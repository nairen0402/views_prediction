{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "#API_KEY =  'AIzaSyDPHaDiN5kPC23gymzo_J497dVmh5lU65E'  # polybuffer\n",
    "API_KEY = 'AIzaSyArMFYxvmiHMWK6hmj4qHPAwYf_00ZoGVA'\n",
    "# API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'#nairen_2\n",
    "# API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs'#nairen_1\n",
    "# API_KEY = 'AIzaSyArMFYxvmiHMWK6hmj4qHPAwYf_00ZoGVA'#daniel\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çˆ¬å–å½±ç‰‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§£ævideo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                             æ‰¹é‡å­˜å–å½±ç‰‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # æ·»åŠ  video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # åœ¨æ¯æ¬¡å¤„ç†ä¸€æ‰¹è§†é¢‘åå»¶æ—¶\n",
    "        time.sleep(2.0)  # å»¶æ—¶ 2 ç§’\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Number of Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=500):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¨‚é–±æ•¸è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”±handleä¾†æ‰¾channelè³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"ç„¡æ³•æ‰¾åˆ°é »é“ï¼Œè«‹æª¢æŸ¥ Handle æ˜¯å¦æ­£ç¢ºã€‚\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API éŒ¯èª¤ï¼š{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_travel_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # ä½¿ç”¨ tqdm é€²åº¦æ¢\n",
    "    for handle in tqdm(handles, desc=\"è™•ç†é »é“é€²åº¦\", unit=\"é »é“\"):\n",
    "        print(f\"æ­£åœ¨è™•ç†é »é“: {handle}\")\n",
    "        # ç²å–é »é“ ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"ç„¡æ³•æ‰¾åˆ°é »é“: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # ç²å–é »é“è¦–é »æ•¸æ“š\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"æœªèƒ½ç²å– {handle} çš„ä»»ä½•è¦–é »æ•¸æ“šã€‚\")\n",
    "            continue\n",
    "\n",
    "        # ç²å–è¨‚é–±æ•¸\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # å°‡æ•¸æ“šè¿½åŠ åˆ°ç¸½æ•¸æ“šä¸­\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # ä½¿ç”¨ tqdm é€²åº¦æ¢å¯«å…¥æ•¸æ“š\n",
    "    print(\"æ­£åœ¨ä¿å­˜æ•¸æ“šåˆ° CSV æ–‡ä»¶...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"ä¿å­˜æ•¸æ“šé€²åº¦\", unit=\"å½±ç‰‡\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‰µå»ºä¸¦å¯«å…¥è¡¨é ­\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ æ•¸æ“šï¼ˆé¿å…é‡è¤‡ï¼‰\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸä¿å­˜åˆ° CSV æ–‡ä»¶ï¼š\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†é »é“é€²åº¦:   0%|          | 0/1 [00:00<?, ?é »é“/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†é »é“: @JapanTravelVlogMrG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†é »é“é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.66s/é »é“]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¿å­˜æ•¸æ“šåˆ° CSV æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¿å­˜æ•¸æ“šé€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 7795.18å½±ç‰‡/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸä¿å­˜åˆ° CSV æ–‡ä»¶ï¼š\n",
      "        video_id                                              title   views  \\\n",
      "0    JyRxnHT4HDY                                          ç¬¬ä¸€æ¬¡å¸å¦çµ¦å¤§å®¶çœ‹   70699   \n",
      "1    dRSoKZM59nw                               å°ç¤¾ç•œçš„åœ°ä½ï¼Œé€™æ¨£æ±ºå®šé›»æœ€å°‘çš„æ°¸é æ˜¯è´å®¶   21630   \n",
      "2    tCbSd2ceoRQ                                     æœ­å¹Œé®­é­šåµè“‹é£¯åŠ åˆ°æ»¿çš„å‡ºä¾†ï¼   24985   \n",
      "3    ufnMZWBKE2E                                   ä½³é‡Œ 8 å®¶ã€Œåœ¨åœ°ä¸€ç´šæ£’ç¾é£Ÿã€ï¼   38122   \n",
      "4    OZXwLhutpZU                           ç´€å¿µä¸€ä¸‹é¦–é£›ï¼ï¼ç¶“éä¸€å€‹å¤šæœˆçš„èª²ç¨‹çµ‚æ–¼èµ·é£›äº†ğŸ›«ï¸   84847   \n",
      "..           ...                                                ...     ...   \n",
      "597  0GtSUxBxqEU  ã€æ­æ´²æ—…éŠã€‘ç¨é—–å¾·åœ‹æœ€å¤§ç´…ç‡ˆå€ãƒ»æ¼¢å ¡ç¹©ç´¢è¡—Reeperbahnãƒ»å¾·åœ‹æ¼¢å ¡æ™¯é»è§€å…‰ãƒ»æ¼¢å ¡å¸‚é›†è·³...  757581   \n",
      "598  CL5rzys5l94  ã€è¶Šå—æ—…éŠã€‘2023å¹´è¶Šå—è‡ªç”±è¡Œep1ãƒ»èƒ¡å¿—æ˜è¥¿è²¢çš„ç¬¬ä¸€å¤©ãƒ»å¤œç”Ÿæ´»å¤ªéœ‡é©šï¼ Ho Chi M...   61921   \n",
      "599  jMdKGrKTjYc  ã€è¶Šå—æ—…éŠã€‘åªè¦60ç¾é‡‘çš„æœƒå®‰è¶…è´Šåº¦å‡æ‘ãƒ»æœƒå®‰é£¯åº—æ¨è–¦ãƒ»æœƒå®‰é…’åº—æ¨è–¦ãƒ»æœƒå®‰æ—…é¤¨æ¨è–¦ãƒ»2023...   14966   \n",
      "600  CB7H14MdLHw  ã€æ—¥æœ¬æ—…éŠã€‘æ¢è¨ªæ—¥æœ¬â€œè‡ªæ®ºæ£®æ—â€é’æœ¨åŸæ¨¹æµ·ãƒ»çœŸçš„å¯æ€•å—ï¼Ÿ 2023å¹´æ—¥æœ¬å±±æ¢¨ç¸£å¯Œå£«å±±é€±é‚Šè‡ªç”±...   35118   \n",
      "601  C_Ar1fT49OA  ã€æ—¥æœ¬æ—…éŠã€‘2021ç§‹äº¬éƒ½åµå±±ç´…è‘‰å­£ï½œå¤©é¾å¯ºÂ·å¯¶åš´é™¢Â·æ¸¡æœˆæ©‹Â·åµ¯å³¨é‡Â·ä¿æ´¥å³½è³æ¥“ [4K V...    4618   \n",
      "\n",
      "     likes  comments  publish_days  subscribers  \n",
      "0     1181        37           687       543000  \n",
      "1      340       126           635       543000  \n",
      "2        0        11           692       543000  \n",
      "3      887        16           669       543000  \n",
      "4     3326        69           559       543000  \n",
      "..     ...       ...           ...          ...  \n",
      "597   3832       164           510       266000  \n",
      "598    526        28           670       266000  \n",
      "599    200        12           615       266000  \n",
      "600    561        41           552       266000  \n",
      "601     82        16          1129       266000  \n",
      "\n",
      "[602 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handles = ['@JapanTravelVlogMrG']  #youtube ä¸»é ç¶²å€\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport time \\nimport os\\nfrom googleapiclient.discovery import build\\n\\nAPI_KEY = \\'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8\\'\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\n# é€²åº¦å„²å­˜æª”æ¡ˆ\\nPROGRESS_FILE = \\'search_progress.json\\'\\n\\ndef save_progress(handles, next_page_token):\\n    \"\"\"å„²å­˜æœå°‹é€²åº¦åˆ°æª”æ¡ˆ\"\"\"\\n    progress_data = {\\n        \"handles\": sorted(list(handles)),  # ç¢ºä¿é †åºä¸”ç„¡é‡è¤‡\\n        \"nextPageToken\": next_page_token\\n    }\\n    with open(PROGRESS_FILE, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(progress_data, f, ensure_ascii=False, indent=4)\\n\\ndef load_progress():\\n    \"\"\"è¼‰å…¥æœå°‹é€²åº¦\"\"\"\\n    if os.path.exists(PROGRESS_FILE):\\n        with open(PROGRESS_FILE, \\'r\\', encoding=\\'utf-8\\') as f:\\n            progress_data = json.load(f)\\n            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\\n    return set(), None\\n\\ndef get_handles_from_keyword(keyword, max_results=50):\\n    handles, next_page_token = load_progress()\\n\\n    while len(handles) < max_results:\\n        # æœå°‹é »é“\\n        search_request = youtube.search().list(\\n            part=\\'snippet\\',\\n            q=keyword,\\n            type=\\'channel\\',\\n            maxResults=50,\\n            pageToken=next_page_token\\n        )\\n        search_response = search_request.execute()\\n\\n        for item in search_response[\\'items\\']:\\n            channel_id = item[\\'snippet\\'][\\'channelId\\']\\n\\n            # åœ¨æ¯æ¬¡è°ƒç”¨é¢‘é“ API å‰åŠ å…¥å»¶æ—¶\\n            time.sleep(3.0)  # å»¶æ—¶ 1.5 ç§’\\n\\n            # ç²å–é »é“è³‡è¨Šä»¥æå– handle\\n            channel_request = youtube.channels().list(\\n                part=\\'snippet\\',\\n                id=channel_id\\n            )\\n            channel_response = channel_request.execute()\\n\\n            for channel in channel_response[\\'items\\']:\\n                custom_url = channel[\\'snippet\\'].get(\\'customUrl\\', None)\\n                title = channel[\\'snippet\\'][\\'title\\']\\n                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(\\' \\', \\'\\').replace(\\'-\\', \\'\\')}\"\\n                handles.add(handle)\\n\\n        # å„²å­˜é€²åº¦\\n        next_page_token = search_response.get(\\'nextPageToken\\')\\n        save_progress(handles, next_page_token)\\n\\n        if not next_page_token:\\n            break\\n\\n        # åœ¨æ¯æ¬¡åˆ†é¡µè¯·æ±‚ä¹‹é—´å»¶æ—¶\\n        time.sleep(3.0)  \\n\\n    return sorted(list(handles))[:max_results]\\n\\n\\n# æœå°‹å°ç£ç›¸é—œçš„é »é“\\nkeyword = \"å°ç£\"\\nhandles = get_handles_from_keyword(keyword, max_results=10)\\nprint(\"æ‰¾åˆ°çš„ Handles:\")\\nbatch_main(handles)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# é€²åº¦å„²å­˜æª”æ¡ˆ\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"å„²å­˜æœå°‹é€²åº¦åˆ°æª”æ¡ˆ\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # ç¢ºä¿é †åºä¸”ç„¡é‡è¤‡\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"è¼‰å…¥æœå°‹é€²åº¦\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # æœå°‹é »é“\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # åœ¨æ¯æ¬¡è°ƒç”¨é¢‘é“ API å‰åŠ å…¥å»¶æ—¶\n",
    "            time.sleep(3.0)  # å»¶æ—¶ 1.5 ç§’\n",
    "\n",
    "            # ç²å–é »é“è³‡è¨Šä»¥æå– handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # å„²å­˜é€²åº¦\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # åœ¨æ¯æ¬¡åˆ†é¡µè¯·æ±‚ä¹‹é—´å»¶æ—¶\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# æœå°‹å°ç£ç›¸é—œçš„é »é“\n",
    "keyword = \"å°ç£\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"æ‰¾åˆ°çš„ Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è³‡æ–™è™•ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æå–**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_45940\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4440.  1825.5  979.5 ...   12.     4.5   61.5]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n",
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_45940\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[208800. 208800. 208800. ...  42320.  42320.  42320.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 243445620054.7161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "data = pd.read_csv(\"youtube_travel_data.csv\")\n",
    "features = data[['likes', 'comments', 'publish_days', 'subscribers']]\n",
    "target = data['views']\n",
    "weights = {'likes': 1.5, 'comments': 1.0, 'publish_days': 1.0, 'subscribers': 0.8}\n",
    "for feature, weight in weights.items():\n",
    "    features.loc[:, feature] = features[feature] * weight\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å»ºæ¨¡**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å·²è¨“ç·´å®Œæˆï¼Œå‡æ–¹èª¤å·® (MSE): 243445620054.7161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°ï¼ˆå¯é¸ï¼Œç”¨æ–¼ç¢ºèªæ¨¡å‹æ•ˆæœï¼‰\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"æ¨¡å‹å·²è¨“ç·´å®Œæˆï¼Œå‡æ–¹èª¤å·® (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é æ¸¬çš„ views ç‚º: 191905.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "likes = 3792\n",
    "comments = 185\n",
    "publish_days = 102\n",
    "subscribers = 343000\n",
    "input_data = pd.DataFrame([[likes, comments, publish_days, subscribers]], \n",
    "                          columns=['likes', 'comments', 'publish_days', 'subscribers'])\n",
    "\n",
    "predicted_views = model.predict(input_data)[0]\n",
    "print(f\"é æ¸¬çš„ views ç‚º: {predicted_views}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
