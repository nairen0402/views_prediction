{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "#API_KEY =  'AIzaSyDPHaDiN5kPC23gymzo_J497dVmh5lU65E'  # polybuffer\n",
    "API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs' \n",
    "# API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'#nairen_2\n",
    "# API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs'#nairen_1\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                             批量存取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # 添加 video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # 在每次处理一批视频后延时\n",
    "        time.sleep(2.0)  # 延时 2 秒\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Number of Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=500):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訂閱數資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 由handle來找channel資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"無法找到頻道，請檢查 Handle 是否正確。\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API 錯誤：{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_travel_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # 使用 tqdm 進度條\n",
    "    for handle in tqdm(handles, desc=\"處理頻道進度\", unit=\"頻道\"):\n",
    "        print(f\"正在處理頻道: {handle}\")\n",
    "        # 獲取頻道 ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"無法找到頻道: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # 獲取頻道視頻數據\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"未能獲取 {handle} 的任何視頻數據。\")\n",
    "            continue\n",
    "\n",
    "        # 獲取訂閱數\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # 將數據追加到總數據中\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # 使用 tqdm 進度條寫入數據\n",
    "    print(\"正在保存數據到 CSV 文件...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"保存數據進度\", unit=\"影片\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # 如果文件不存在，創建並寫入表頭\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # 如果文件存在，追加數據（避免重複）\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"所有數據已成功保存到 CSV 文件：\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/1 [00:00<?, ?頻道/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理頻道: @胡子Huzi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/1 [00:00<?, ?頻道/s]\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id&channelId=UC9YOQFPfEUXbulKDtxeqqBA&maxResults=50&key=AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m handles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@胡子Huzi\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m#youtube 主頁網址\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbatch_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 19\u001b[0m, in \u001b[0;36mbatch_main\u001b[1;34m(handles, csv_file)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 獲取頻道視頻數據\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m video_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_channel_video_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m video_data:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m未能獲取 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhandle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 的任何視頻數據。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m, in \u001b[0;36mget_channel_video_data\u001b[1;34m(channel_id, max_results)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_video_data\u001b[39m(channel_id, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     video_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_ids_from_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     video_data \u001b[38;5;241m=\u001b[39m batch_video_data(video_ids)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m video_data\n",
      "Cell \u001b[1;32mIn[53], line 13\u001b[0m, in \u001b[0;36mget_video_ids_from_channel\u001b[1;34m(channel_id, max_results)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(video_ids) \u001b[38;5;241m<\u001b[39m max_results:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Fetching at most 50 results at a time\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m      8\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m         channelId\u001b[38;5;241m=\u001b[39mchannel_id,\n\u001b[0;32m     10\u001b[0m         maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     11\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[0;32m     12\u001b[0m     )\n\u001b[1;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Extract video IDs\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id&channelId=UC9YOQFPfEUXbulKDtxeqqBA&maxResults=50&key=AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "handles = ['@胡子Huzi']  #youtube 主頁網址\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport time \\nimport os\\nfrom googleapiclient.discovery import build\\n\\nAPI_KEY = \\'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8\\'\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\n# 進度儲存檔案\\nPROGRESS_FILE = \\'search_progress.json\\'\\n\\ndef save_progress(handles, next_page_token):\\n    \"\"\"儲存搜尋進度到檔案\"\"\"\\n    progress_data = {\\n        \"handles\": sorted(list(handles)),  # 確保順序且無重複\\n        \"nextPageToken\": next_page_token\\n    }\\n    with open(PROGRESS_FILE, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(progress_data, f, ensure_ascii=False, indent=4)\\n\\ndef load_progress():\\n    \"\"\"載入搜尋進度\"\"\"\\n    if os.path.exists(PROGRESS_FILE):\\n        with open(PROGRESS_FILE, \\'r\\', encoding=\\'utf-8\\') as f:\\n            progress_data = json.load(f)\\n            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\\n    return set(), None\\n\\ndef get_handles_from_keyword(keyword, max_results=50):\\n    handles, next_page_token = load_progress()\\n\\n    while len(handles) < max_results:\\n        # 搜尋頻道\\n        search_request = youtube.search().list(\\n            part=\\'snippet\\',\\n            q=keyword,\\n            type=\\'channel\\',\\n            maxResults=50,\\n            pageToken=next_page_token\\n        )\\n        search_response = search_request.execute()\\n\\n        for item in search_response[\\'items\\']:\\n            channel_id = item[\\'snippet\\'][\\'channelId\\']\\n\\n            # 在每次调用频道 API 前加入延时\\n            time.sleep(3.0)  # 延时 1.5 秒\\n\\n            # 獲取頻道資訊以提取 handle\\n            channel_request = youtube.channels().list(\\n                part=\\'snippet\\',\\n                id=channel_id\\n            )\\n            channel_response = channel_request.execute()\\n\\n            for channel in channel_response[\\'items\\']:\\n                custom_url = channel[\\'snippet\\'].get(\\'customUrl\\', None)\\n                title = channel[\\'snippet\\'][\\'title\\']\\n                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(\\' \\', \\'\\').replace(\\'-\\', \\'\\')}\"\\n                handles.add(handle)\\n\\n        # 儲存進度\\n        next_page_token = search_response.get(\\'nextPageToken\\')\\n        save_progress(handles, next_page_token)\\n\\n        if not next_page_token:\\n            break\\n\\n        # 在每次分页请求之间延时\\n        time.sleep(3.0)  \\n\\n    return sorted(list(handles))[:max_results]\\n\\n\\n# 搜尋台灣相關的頻道\\nkeyword = \"台灣\"\\nhandles = get_handles_from_keyword(keyword, max_results=10)\\nprint(\"找到的 Handles:\")\\nbatch_main(handles)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# 進度儲存檔案\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"儲存搜尋進度到檔案\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # 確保順序且無重複\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"載入搜尋進度\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # 搜尋頻道\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # 在每次调用频道 API 前加入延时\n",
    "            time.sleep(3.0)  # 延时 1.5 秒\n",
    "\n",
    "            # 獲取頻道資訊以提取 handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # 儲存進度\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # 在每次分页请求之间延时\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# 搜尋台灣相關的頻道\n",
    "keyword = \"台灣\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"找到的 Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_25748\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4440.  1825.5  979.5 ... 3195.  1056.   349.5]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n",
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_25748\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[208800. 208800. 208800. ... 686400. 686400. 686400.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 257344564432.03488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "data = pd.read_csv(\"youtube_video_data.csv\")\n",
    "features = data[['likes', 'comments', 'publish_days', 'subscribers']]\n",
    "target = data['views']\n",
    "weights = {'likes': 1.5, 'comments': 1.0, 'publish_days': 1.0, 'subscribers': 0.8}\n",
    "for feature, weight in weights.items():\n",
    "    features.loc[:, feature] = features[feature] * weight\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建模**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已訓練完成，均方誤差 (MSE): 257344564432.03488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 建立模型\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估（可選，用於確認模型效果）\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"模型已訓練完成，均方誤差 (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測的 views 為: 198184.68\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "likes = 3792\n",
    "comments = 185\n",
    "publish_days = 102\n",
    "subscribers = 343000\n",
    "input_data = pd.DataFrame([[likes, comments, publish_days, subscribers]], \n",
    "                          columns=['likes', 'comments', 'publish_days', 'subscribers'])\n",
    "\n",
    "predicted_views = model.predict(input_data)[0]\n",
    "print(f\"預測的 views 為: {predicted_views}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
