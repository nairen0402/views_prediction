{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs' # nairen\n",
    "#API_KEY = 'AIzaSyDPHaDiN5kPC23gymzo_J497dVmh5lU65E'  # polybuffer\n",
    "# API_KEY = #daniel\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                             批量存取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # 添加 video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # 在每次处理一批视频后延时\n",
    "        time.sleep(2.0)  # 延时 2 秒\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Number of Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=500):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訂閱數資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 由handle來找channel資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"無法找到頻道，請檢查 Handle 是否正確。\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API 錯誤：{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_video_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # 使用 tqdm 進度條\n",
    "    for handle in tqdm(handles, desc=\"處理頻道進度\", unit=\"頻道\"):\n",
    "        print(f\"正在處理頻道: {handle}\")\n",
    "        # 獲取頻道 ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"無法找到頻道: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # 獲取頻道視頻數據\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"未能獲取 {handle} 的任何視頻數據。\")\n",
    "            continue\n",
    "\n",
    "        # 獲取訂閱數\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # 將數據追加到總數據中\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # 使用 tqdm 進度條寫入數據\n",
    "    print(\"正在保存數據到 CSV 文件...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"保存數據進度\", unit=\"影片\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # 如果文件不存在，創建並寫入表頭\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # 如果文件存在，追加數據（避免重複）\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"所有數據已成功保存到 CSV 文件：\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/1 [00:00<?, ?頻道/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理頻道: @wei-teng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度: 100%|██████████| 1/1 [00:20<00:00, 20.08s/頻道]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在保存數據到 CSV 文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "保存數據進度: 100%|██████████| 386/386 [00:00<00:00, 3243.67影片/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有數據已成功保存到 CSV 文件：\n",
      "          video_id                            title   views  likes  comments  \\\n",
      "0      FgLkUspIsL0     全職獵人405話完整解說：西索本尊現身，旅團黑幫皆有行動  256441   2960       504   \n",
      "1      Fy_bHNbWdRU  拳叔講哆啦A夢：哆啦A夢把房子變身無敵列車頭！開著房子去兜風！  154404   1217        31   \n",
      "2      61fYLew16O8    臥底名單洩露，庫拉索的選擇，柯南劇場版20 純黑的噩夢解說   77427    653        69   \n",
      "3      SHyMV8kt3F0          獵人解說2:少數服從多數的陷阱，西索的放養計劃  239759   1263       150   \n",
      "4      Htyr-4aOqe0      一反常态的基德，解救业火中的向日葵，柯南剧场版19解说   71886    557        64   \n",
      "...            ...                              ...     ...    ...       ...   \n",
      "12880  3lmEWlOndB4                    微疼首款換頭扭蛋來辣!!!  168181   4415       202   \n",
      "12881  RF-fT7ioxcI             【兩件事情宣布】關於投稿還有故事被盜用了  407510  13781      1065   \n",
      "12882  V85dsRQoRnA                  【微動畫】這是一部，低成本動畫  181533   3576       281   \n",
      "12883  L0HfvqHEPuA             北海道員工DAY1|第一次來就上手的行程  108453   2124        78   \n",
      "12884  dVg0_v2d7U4                   說好的尷尬配音片...|微疼  118765   5046       249   \n",
      "\n",
      "       publish_days  subscribers  \n",
      "0                35       261000  \n",
      "1              1849       261000  \n",
      "2               319       261000  \n",
      "3              1053       261000  \n",
      "4               325       261000  \n",
      "...             ...          ...  \n",
      "12880          1423      1120000  \n",
      "12881          1684      1120000  \n",
      "12882          2312      1120000  \n",
      "12883           308      1120000  \n",
      "12884          2285      1120000  \n",
      "\n",
      "[12885 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handles = ['@wei-teng']  #youtube 主頁網址\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport time \\nimport os\\nfrom googleapiclient.discovery import build\\n\\nAPI_KEY = \\'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8\\'\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\n# 進度儲存檔案\\nPROGRESS_FILE = \\'search_progress.json\\'\\n\\ndef save_progress(handles, next_page_token):\\n    \"\"\"儲存搜尋進度到檔案\"\"\"\\n    progress_data = {\\n        \"handles\": sorted(list(handles)),  # 確保順序且無重複\\n        \"nextPageToken\": next_page_token\\n    }\\n    with open(PROGRESS_FILE, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(progress_data, f, ensure_ascii=False, indent=4)\\n\\ndef load_progress():\\n    \"\"\"載入搜尋進度\"\"\"\\n    if os.path.exists(PROGRESS_FILE):\\n        with open(PROGRESS_FILE, \\'r\\', encoding=\\'utf-8\\') as f:\\n            progress_data = json.load(f)\\n            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\\n    return set(), None\\n\\ndef get_handles_from_keyword(keyword, max_results=50):\\n    handles, next_page_token = load_progress()\\n\\n    while len(handles) < max_results:\\n        # 搜尋頻道\\n        search_request = youtube.search().list(\\n            part=\\'snippet\\',\\n            q=keyword,\\n            type=\\'channel\\',\\n            maxResults=50,\\n            pageToken=next_page_token\\n        )\\n        search_response = search_request.execute()\\n\\n        for item in search_response[\\'items\\']:\\n            channel_id = item[\\'snippet\\'][\\'channelId\\']\\n\\n            # 在每次调用频道 API 前加入延时\\n            time.sleep(3.0)  # 延时 1.5 秒\\n\\n            # 獲取頻道資訊以提取 handle\\n            channel_request = youtube.channels().list(\\n                part=\\'snippet\\',\\n                id=channel_id\\n            )\\n            channel_response = channel_request.execute()\\n\\n            for channel in channel_response[\\'items\\']:\\n                custom_url = channel[\\'snippet\\'].get(\\'customUrl\\', None)\\n                title = channel[\\'snippet\\'][\\'title\\']\\n                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(\\' \\', \\'\\').replace(\\'-\\', \\'\\')}\"\\n                handles.add(handle)\\n\\n        # 儲存進度\\n        next_page_token = search_response.get(\\'nextPageToken\\')\\n        save_progress(handles, next_page_token)\\n\\n        if not next_page_token:\\n            break\\n\\n        # 在每次分页请求之间延时\\n        time.sleep(3.0)  \\n\\n    return sorted(list(handles))[:max_results]\\n\\n\\n# 搜尋台灣相關的頻道\\nkeyword = \"台灣\"\\nhandles = get_handles_from_keyword(keyword, max_results=10)\\nprint(\"找到的 Handles:\")\\nbatch_main(handles)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# 進度儲存檔案\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"儲存搜尋進度到檔案\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # 確保順序且無重複\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"載入搜尋進度\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # 搜尋頻道\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # 在每次调用频道 API 前加入延时\n",
    "            time.sleep(3.0)  # 延时 1.5 秒\n",
    "\n",
    "            # 獲取頻道資訊以提取 handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # 儲存進度\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # 在每次分页请求之间延时\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# 搜尋台灣相關的頻道\n",
    "keyword = \"台灣\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"找到的 Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "data = pd.read_csv(\"youtube_video_data.csv\")\n",
    "features = data[['likes', 'comments', 'publish_days', 'subscribers']]\n",
    "target = data['views']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建模**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已訓練完成，均方誤差 (MSE): 40241720971.86998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 建立模型\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估（可選，用於確認模型效果）\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"模型已訓練完成，均方誤差 (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測的 views 為: 102548.15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "likes = 1202\n",
    "comments = 104\n",
    "publish_days = 712\n",
    "subscribers = 56400\n",
    "input_data = pd.DataFrame([[likes, comments, publish_days, subscribers]], \n",
    "                          columns=['likes', 'comments', 'publish_days', 'subscribers'])\n",
    "\n",
    "predicted_views = model.predict(input_data)[0]\n",
    "print(f\"預測的 views 為: {predicted_views}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
