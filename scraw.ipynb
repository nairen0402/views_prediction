{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çˆ¬å–å½±ç‰‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§£ævideo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                             æ‰¹é‡å­˜å–å½±ç‰‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # æ·»åŠ  video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # åœ¨æ¯æ¬¡å¤„ç†ä¸€æ‰¹è§†é¢‘åå»¶æ—¶\n",
    "        time.sleep(2.0)  # å»¶æ—¶ 2 ç§’\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=5):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¨‚é–±æ•¸è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”±handleä¾†æ‰¾channelè³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"ç„¡æ³•æ‰¾åˆ°é »é“ï¼Œè«‹æª¢æŸ¥ Handle æ˜¯å¦æ­£ç¢ºã€‚\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API éŒ¯èª¤ï¼š{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_video_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # ä½¿ç”¨ tqdm é€²åº¦æ¢\n",
    "    for handle in tqdm(handles, desc=\"è™•ç†é »é“é€²åº¦\", unit=\"é »é“\"):\n",
    "        print(f\"æ­£åœ¨è™•ç†é »é“: {handle}\")\n",
    "        # ç²å–é »é“ ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"ç„¡æ³•æ‰¾åˆ°é »é“: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # ç²å–é »é“è¦–é »æ•¸æ“š\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"æœªèƒ½ç²å– {handle} çš„ä»»ä½•è¦–é »æ•¸æ“šã€‚\")\n",
    "            continue\n",
    "\n",
    "        # ç²å–è¨‚é–±æ•¸\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # å°‡æ•¸æ“šè¿½åŠ åˆ°ç¸½æ•¸æ“šä¸­\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # ä½¿ç”¨ tqdm é€²åº¦æ¢å¯«å…¥æ•¸æ“š\n",
    "    print(\"æ­£åœ¨ä¿å­˜æ•¸æ“šåˆ° CSV æ–‡ä»¶...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"ä¿å­˜æ•¸æ“šé€²åº¦\", unit=\"å½±ç‰‡\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‰µå»ºä¸¦å¯«å…¥è¡¨é ­\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ æ•¸æ“šï¼ˆé¿å…é‡è¤‡ï¼‰\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸä¿å­˜åˆ° CSV æ–‡ä»¶ï¼š\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†é »é“é€²åº¦:   0%|          | 0/1 [00:00<?, ?é »é“/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†é »é“: @ä½•é›»å» \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†é »é“é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.44s/é »é“]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¿å­˜æ•¸æ“šåˆ° CSV æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¿å­˜æ•¸æ“šé€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1000.88å½±ç‰‡/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸä¿å­˜åˆ° CSV æ–‡ä»¶ï¼š\n",
      "       video_id                                      title   views  likes  \\\n",
      "0   FgLkUspIsL0               å…¨è·çµäºº405è©±å®Œæ•´è§£èªªï¼šè¥¿ç´¢æœ¬å°Šç¾èº«ï¼Œæ—…åœ˜é»‘å¹«çš†æœ‰è¡Œå‹•  256441   2960   \n",
      "1   Fy_bHNbWdRU            æ‹³å”è¬›å“†å•¦Aå¤¢ï¼šå“†å•¦Aå¤¢æŠŠæˆ¿å­è®Šèº«ç„¡æ•µåˆ—è»Šé ­ï¼é–‹è‘—æˆ¿å­å»å…œé¢¨ï¼  154404   1217   \n",
      "2   61fYLew16O8              è‡¥åº•åå–®æ´©éœ²ï¼Œåº«æ‹‰ç´¢çš„é¸æ“‡ï¼ŒæŸ¯å—åŠ‡å ´ç‰ˆ20 ç´”é»‘çš„å™©å¤¢è§£èªª   77427    653   \n",
      "3   SHyMV8kt3F0                    çµäººè§£èªª2:å°‘æ•¸æœå¾å¤šæ•¸çš„é™·é˜±ï¼Œè¥¿ç´¢çš„æ”¾é¤Šè¨ˆåŠƒ  239759   1263   \n",
      "4   Htyr-4aOqe0                ä¸€åå¸¸æ€çš„åŸºå¾·ï¼Œè§£æ•‘ä¸šç«ä¸­çš„å‘æ—¥è‘µï¼ŒæŸ¯å—å‰§åœºç‰ˆ19è§£è¯´   71886    557   \n",
      "5   3L97GdpRfmk                                          ğŸ€   23026    490   \n",
      "6   mfZCP0mWAcY                   ç¾é£Ÿçµäººè§£èªª37ï¼šé‡‘ä¹‹ä¸»å»šå‰é›†ç¾èº«ï¼Œçœ¾äººåˆ†é“æšé‘£   54094    507   \n",
      "7   -g4cZKqvyi8                          å…¨è·çµäºº408è©±è¨è«–ï¼šå¡ä¸èˆ‡å¢¨è“®å¨œ   54328    728   \n",
      "8   hcuSnYbEXZU               å…¨è·çµäºº406è©±å®Œæ•´è§£èªªï¼šåœ˜é•·ç›¯ä¸Šå¡ä¸ä¸‰å¤§ç¥å™¨ï¼Œé€è‘¬é–‹å§‹  132548   1824   \n",
      "9   UcJSqVdISzk                                      æ¯›å·¾æ‹¿ä¾†ï¼  204045   6996   \n",
      "10  4YZClY7CJcs                                  12å¼·å°ç£å¥ªå† ç¬é–“  743774  23520   \n",
      "11  mGGeepjpDxU         å¤©æ°£ç‚ç†± æ–°èŠæ£’çƒå ´æ¶¼æ°´æ‹›å¾… #baseball #cpblä¸­è¯è·æ£’  151750   3444   \n",
      "12  1tliW-0LwhI                      é€²å ´çœ‹çƒè¢«èŠæ©ç™¼ç¾ï¼å¥½å¿ƒæ¨è–¦æˆ‘æ›´å¥½çš„åº§ä½ï¼Ÿ   87957   2492   \n",
      "13  jqzlrZE1zRw                      å¥¶æ˜”Go Stronger 32æ’æ¨¡ä»¿ç§€ï¼  245798   7780   \n",
      "14  0o7CU7cR9kc                             å¥½çš„æ¨‚æ„æ•ˆå‹/ã¯ã„ã‚ˆã‚ã“ã‚“ã§   78967   3336   \n",
      "15  kHz9zhUSENQ                           ï¼»æœƒè€ƒå»ºè­°ï¼½æº«é¦¨æé†’èˆ‡ç¥ˆç¦å°æ’‡æ­¥   22283   1207   \n",
      "16  LOSktdqblQM                                æ–°ç«¹å­¸ç”Ÿå¿…åƒç¾é£ŸEP3   11279    447   \n",
      "17  jPbsGOc4uQo                                     èºé¬±é¾œå…”è³½è·‘   11080    393   \n",
      "18  wNbCFgozll8  æˆ‘å°é›åŸæ­¦åˆ†æJoemanåˆ†æYouTubeæµé‡é€™éš»ç‰‡é€™éš»ç‰‡çš„çœ‹æ³•ï¼ˆæ²’æœ‰CCå­—å¹•ï¼‰   12051    203   \n",
      "19  97HUqpIcNds                                  ä½ çŸ¥é“é€™æ˜¯ä»€éº¼å—ï¼Ÿ   51889   1878   \n",
      "\n",
      "    comments  publish_days  subscribers  \n",
      "0        504            35       261000  \n",
      "1         31          1849       261000  \n",
      "2         69           319       261000  \n",
      "3        150          1053       261000  \n",
      "4         64           325       261000  \n",
      "5         32           848       261000  \n",
      "6         71           607       261000  \n",
      "7        149             8       261000  \n",
      "8        387            28       261000  \n",
      "9        143            79       432000  \n",
      "10       386             9       432000  \n",
      "11        86           146       432000  \n",
      "12        56           108       432000  \n",
      "13       251            69       432000  \n",
      "14        87           102       432000  \n",
      "15        74           936        41200  \n",
      "16         7           631        41200  \n",
      "17        31           946        41200  \n",
      "18        21           434        41200  \n",
      "19       119           895        41200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handles = ['@ä½•é›»å» ']\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport time \\nimport os\\nfrom googleapiclient.discovery import build\\n\\nAPI_KEY = \\'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8\\'\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\n# é€²åº¦å„²å­˜æª”æ¡ˆ\\nPROGRESS_FILE = \\'search_progress.json\\'\\n\\ndef save_progress(handles, next_page_token):\\n    \"\"\"å„²å­˜æœå°‹é€²åº¦åˆ°æª”æ¡ˆ\"\"\"\\n    progress_data = {\\n        \"handles\": sorted(list(handles)),  # ç¢ºä¿é †åºä¸”ç„¡é‡è¤‡\\n        \"nextPageToken\": next_page_token\\n    }\\n    with open(PROGRESS_FILE, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(progress_data, f, ensure_ascii=False, indent=4)\\n\\ndef load_progress():\\n    \"\"\"è¼‰å…¥æœå°‹é€²åº¦\"\"\"\\n    if os.path.exists(PROGRESS_FILE):\\n        with open(PROGRESS_FILE, \\'r\\', encoding=\\'utf-8\\') as f:\\n            progress_data = json.load(f)\\n            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\\n    return set(), None\\n\\ndef get_handles_from_keyword(keyword, max_results=50):\\n    handles, next_page_token = load_progress()\\n\\n    while len(handles) < max_results:\\n        # æœå°‹é »é“\\n        search_request = youtube.search().list(\\n            part=\\'snippet\\',\\n            q=keyword,\\n            type=\\'channel\\',\\n            maxResults=50,\\n            pageToken=next_page_token\\n        )\\n        search_response = search_request.execute()\\n\\n        for item in search_response[\\'items\\']:\\n            channel_id = item[\\'snippet\\'][\\'channelId\\']\\n\\n            # åœ¨æ¯æ¬¡è°ƒç”¨é¢‘é“ API å‰åŠ å…¥å»¶æ—¶\\n            time.sleep(3.0)  # å»¶æ—¶ 1.5 ç§’\\n\\n            # ç²å–é »é“è³‡è¨Šä»¥æå– handle\\n            channel_request = youtube.channels().list(\\n                part=\\'snippet\\',\\n                id=channel_id\\n            )\\n            channel_response = channel_request.execute()\\n\\n            for channel in channel_response[\\'items\\']:\\n                custom_url = channel[\\'snippet\\'].get(\\'customUrl\\', None)\\n                title = channel[\\'snippet\\'][\\'title\\']\\n                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(\\' \\', \\'\\').replace(\\'-\\', \\'\\')}\"\\n                handles.add(handle)\\n\\n        # å„²å­˜é€²åº¦\\n        next_page_token = search_response.get(\\'nextPageToken\\')\\n        save_progress(handles, next_page_token)\\n\\n        if not next_page_token:\\n            break\\n\\n        # åœ¨æ¯æ¬¡åˆ†é¡µè¯·æ±‚ä¹‹é—´å»¶æ—¶\\n        time.sleep(3.0)  \\n\\n    return sorted(list(handles))[:max_results]\\n\\n\\n# æœå°‹å°ç£ç›¸é—œçš„é »é“\\nkeyword = \"å°ç£\"\\nhandles = get_handles_from_keyword(keyword, max_results=10)\\nprint(\"æ‰¾åˆ°çš„ Handles:\")\\nbatch_main(handles)\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# é€²åº¦å„²å­˜æª”æ¡ˆ\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"å„²å­˜æœå°‹é€²åº¦åˆ°æª”æ¡ˆ\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # ç¢ºä¿é †åºä¸”ç„¡é‡è¤‡\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"è¼‰å…¥æœå°‹é€²åº¦\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # æœå°‹é »é“\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # åœ¨æ¯æ¬¡è°ƒç”¨é¢‘é“ API å‰åŠ å…¥å»¶æ—¶\n",
    "            time.sleep(3.0)  # å»¶æ—¶ 1.5 ç§’\n",
    "\n",
    "            # ç²å–é »é“è³‡è¨Šä»¥æå– handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # å„²å­˜é€²åº¦\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # åœ¨æ¯æ¬¡åˆ†é¡µè¯·æ±‚ä¹‹é—´å»¶æ—¶\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# æœå°‹å°ç£ç›¸é—œçš„é »é“\n",
    "keyword = \"å°ç£\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"æ‰¾åˆ°çš„ Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
