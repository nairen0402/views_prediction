{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "#API_KEY =  'AIzaSyDPHaDiN5kPC23gymzo_J497dVmh5lU65E'  # polybuffer\n",
    "API_KEY = 'AIzaSyArMFYxvmiHMWK6hmj4qHPAwYf_00ZoGVA'\n",
    "# API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'#nairen_2\n",
    "# API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs'#nairen_1\n",
    "# API_KEY = 'AIzaSyArMFYxvmiHMWK6hmj4qHPAwYf_00ZoGVA'#daniel\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                             批量存取影片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # 添加 video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # 在每次处理一批视频后延时\n",
    "        time.sleep(2.0)  # 延时 2 秒\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Number of Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=500):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訂閱數資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 由handle來找channel資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"無法找到頻道，請檢查 Handle 是否正確。\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API 錯誤：{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_travel_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # 使用 tqdm 進度條\n",
    "    for handle in tqdm(handles, desc=\"處理頻道進度\", unit=\"頻道\"):\n",
    "        print(f\"正在處理頻道: {handle}\")\n",
    "        # 獲取頻道 ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"無法找到頻道: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # 獲取頻道視頻數據\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"未能獲取 {handle} 的任何視頻數據。\")\n",
    "            continue\n",
    "\n",
    "        # 獲取訂閱數\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # 將數據追加到總數據中\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # 使用 tqdm 進度條寫入數據\n",
    "    print(\"正在保存數據到 CSV 文件...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"保存數據進度\", unit=\"影片\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # 如果文件不存在，創建並寫入表頭\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # 如果文件存在，追加數據（避免重複）\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"所有數據已成功保存到 CSV 文件：\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set_Channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/1 [00:00<?, ?頻道/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理頻道: @JapanTravelVlogMrG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度: 100%|██████████| 1/1 [00:07<00:00,  7.66s/頻道]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在保存數據到 CSV 文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "保存數據進度: 100%|██████████| 125/125 [00:00<00:00, 7795.18影片/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有數據已成功保存到 CSV 文件：\n",
      "        video_id                                              title   views  \\\n",
      "0    JyRxnHT4HDY                                          第一次卸妝給大家看   70699   \n",
      "1    dRSoKZM59nw                               小社畜的地位，這樣決定電最少的永遠是贏家   21630   \n",
      "2    tCbSd2ceoRQ                                     札幌鮭魚卵蓋飯加到滿的出來！   24985   \n",
      "3    ufnMZWBKE2E                                   佳里 8 家「在地一級棒美食」！   38122   \n",
      "4    OZXwLhutpZU                           紀念一下首飛！！經過一個多月的課程終於起飛了🛫️   84847   \n",
      "..           ...                                                ...     ...   \n",
      "597  0GtSUxBxqEU  【歐洲旅遊】獨闖德國最大紅燈區・漢堡繩索街Reeperbahn・德國漢堡景點觀光・漢堡市集跳...  757581   \n",
      "598  CL5rzys5l94  【越南旅遊】2023年越南自由行ep1・胡志明西貢的第一天・夜生活太震驚！ Ho Chi M...   61921   \n",
      "599  jMdKGrKTjYc  【越南旅遊】只要60美金的會安超贊度假村・會安飯店推薦・會安酒店推薦・會安旅館推薦・2023...   14966   \n",
      "600  CB7H14MdLHw  【日本旅遊】探訪日本“自殺森林”青木原樹海・真的可怕嗎？ 2023年日本山梨縣富士山週邊自由...   35118   \n",
      "601  C_Ar1fT49OA  【日本旅遊】2021秋京都嵐山紅葉季｜天龍寺·寶嚴院·渡月橋·嵯峨野·保津峽賞楓 [4K V...    4618   \n",
      "\n",
      "     likes  comments  publish_days  subscribers  \n",
      "0     1181        37           687       543000  \n",
      "1      340       126           635       543000  \n",
      "2        0        11           692       543000  \n",
      "3      887        16           669       543000  \n",
      "4     3326        69           559       543000  \n",
      "..     ...       ...           ...          ...  \n",
      "597   3832       164           510       266000  \n",
      "598    526        28           670       266000  \n",
      "599    200        12           615       266000  \n",
      "600    561        41           552       266000  \n",
      "601     82        16          1129       266000  \n",
      "\n",
      "[602 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handles = ['@JapanTravelVlogMrG']  #youtube 主頁網址\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport time \\nimport os\\nfrom googleapiclient.discovery import build\\n\\nAPI_KEY = \\'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8\\'\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\n# 進度儲存檔案\\nPROGRESS_FILE = \\'search_progress.json\\'\\n\\ndef save_progress(handles, next_page_token):\\n    \"\"\"儲存搜尋進度到檔案\"\"\"\\n    progress_data = {\\n        \"handles\": sorted(list(handles)),  # 確保順序且無重複\\n        \"nextPageToken\": next_page_token\\n    }\\n    with open(PROGRESS_FILE, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(progress_data, f, ensure_ascii=False, indent=4)\\n\\ndef load_progress():\\n    \"\"\"載入搜尋進度\"\"\"\\n    if os.path.exists(PROGRESS_FILE):\\n        with open(PROGRESS_FILE, \\'r\\', encoding=\\'utf-8\\') as f:\\n            progress_data = json.load(f)\\n            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\\n    return set(), None\\n\\ndef get_handles_from_keyword(keyword, max_results=50):\\n    handles, next_page_token = load_progress()\\n\\n    while len(handles) < max_results:\\n        # 搜尋頻道\\n        search_request = youtube.search().list(\\n            part=\\'snippet\\',\\n            q=keyword,\\n            type=\\'channel\\',\\n            maxResults=50,\\n            pageToken=next_page_token\\n        )\\n        search_response = search_request.execute()\\n\\n        for item in search_response[\\'items\\']:\\n            channel_id = item[\\'snippet\\'][\\'channelId\\']\\n\\n            # 在每次调用频道 API 前加入延时\\n            time.sleep(3.0)  # 延时 1.5 秒\\n\\n            # 獲取頻道資訊以提取 handle\\n            channel_request = youtube.channels().list(\\n                part=\\'snippet\\',\\n                id=channel_id\\n            )\\n            channel_response = channel_request.execute()\\n\\n            for channel in channel_response[\\'items\\']:\\n                custom_url = channel[\\'snippet\\'].get(\\'customUrl\\', None)\\n                title = channel[\\'snippet\\'][\\'title\\']\\n                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(\\' \\', \\'\\').replace(\\'-\\', \\'\\')}\"\\n                handles.add(handle)\\n\\n        # 儲存進度\\n        next_page_token = search_response.get(\\'nextPageToken\\')\\n        save_progress(handles, next_page_token)\\n\\n        if not next_page_token:\\n            break\\n\\n        # 在每次分页请求之间延时\\n        time.sleep(3.0)  \\n\\n    return sorted(list(handles))[:max_results]\\n\\n\\n# 搜尋台灣相關的頻道\\nkeyword = \"台灣\"\\nhandles = get_handles_from_keyword(keyword, max_results=10)\\nprint(\"找到的 Handles:\")\\nbatch_main(handles)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# 進度儲存檔案\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"儲存搜尋進度到檔案\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # 確保順序且無重複\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"載入搜尋進度\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # 搜尋頻道\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # 在每次调用频道 API 前加入延时\n",
    "            time.sleep(3.0)  # 延时 1.5 秒\n",
    "\n",
    "            # 獲取頻道資訊以提取 handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # 儲存進度\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # 在每次分页请求之间延时\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# 搜尋台灣相關的頻道\n",
    "keyword = \"台灣\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"找到的 Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_45940\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4440.  1825.5  979.5 ...   12.     4.5   61.5]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n",
      "C:\\Users\\jeffi\\AppData\\Local\\Temp\\ipykernel_45940\\3760689822.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[208800. 208800. 208800. ...  42320.  42320.  42320.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[:, feature] = features[feature] * weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 243445620054.7161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "data = pd.read_csv(\"youtube_travel_data.csv\")\n",
    "features = data[['likes', 'comments', 'publish_days', 'subscribers']]\n",
    "target = data['views']\n",
    "weights = {'likes': 1.5, 'comments': 1.0, 'publish_days': 1.0, 'subscribers': 0.8}\n",
    "for feature, weight in weights.items():\n",
    "    features.loc[:, feature] = features[feature] * weight\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建模**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已訓練完成，均方誤差 (MSE): 243445620054.7161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 建立模型\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估（可選，用於確認模型效果）\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"模型已訓練完成，均方誤差 (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測的 views 為: 191905.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "likes = 3792\n",
    "comments = 185\n",
    "publish_days = 102\n",
    "subscribers = 343000\n",
    "input_data = pd.DataFrame([[likes, comments, publish_days, subscribers]], \n",
    "                          columns=['likes', 'comments', 'publish_days', 'subscribers'])\n",
    "\n",
    "predicted_views = model.predict(input_data)[0]\n",
    "print(f\"預測的 views 為: {predicted_views}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
