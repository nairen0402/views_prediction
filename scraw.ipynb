{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "API_KEY = 'AIzaSyBBjHEU2XTcP1hn9GVEHt_f4I9jMSIFaPs'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics, snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None  # Return None if no data\n",
    "\n",
    "    item = response['items'][0]\n",
    "    statistics = item['statistics']\n",
    "    snippet = item['snippet']\n",
    "\n",
    "    # Extracting required data\n",
    "    title = snippet['title']\n",
    "    views = int(statistics.get('viewCount', 0))\n",
    "    likes = int(statistics.get('likeCount', 0))\n",
    "    comments = int(statistics.get('commentCount', 0))\n",
    "    publish_date = snippet['publishedAt']\n",
    "    publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'publish_days': publish_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_from_channel(channel_id, max_results=500):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetching at most 50 results at a time\n",
    "        request = youtube.search().list(\n",
    "            part='id',\n",
    "            channelId=channel_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                video_ids.append(item['id']['videoId'])\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_video_data(video_ids):\n",
    "    data = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics, snippet',\n",
    "            id=','.join(batch_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']\n",
    "            statistics = item['statistics']\n",
    "            publish_date = snippet['publishedAt']\n",
    "            publish_days = (datetime.datetime.now() - datetime.datetime.fromisoformat(publish_date[:-1])).days\n",
    "            views = int(statistics.get('viewCount', 0))\n",
    "\n",
    "            # Filter out videos with 0 views\n",
    "            if views > 0:\n",
    "                data.append({\n",
    "                    'video_id': item['id'],  # 添加 video_id\n",
    "                    'title': snippet['title'],\n",
    "                    'views': views,\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'comments': int(statistics.get('commentCount', 0)),\n",
    "                    'publish_days': publish_days\n",
    "                })\n",
    "\n",
    "        # 在每次处理一批视频后延时\n",
    "        time.sleep(2.0)  # 延时 2 秒\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_video_data(channel_id, max_results=5):\n",
    "    video_ids = get_video_ids_from_channel(channel_id, max_results)\n",
    "    video_data = batch_video_data(video_ids)\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_subscriber_count(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        return None\n",
    "\n",
    "    return int(response['items'][0]['statistics'].get('subscriberCount', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_channel_id_by_handle(handle):\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            forHandle=handle\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['id']\n",
    "        print(\"無法找到頻道，請檢查 Handle 是否正確。\")\n",
    "        return None\n",
    "    except HttpError as e:\n",
    "        print(f\"API 錯誤：{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def batch_main(handles, csv_file='youtube_video_data.csv'):\n",
    "    all_video_data = []\n",
    "\n",
    "    # 使用 tqdm 進度條\n",
    "    for handle in tqdm(handles, desc=\"處理頻道進度\", unit=\"頻道\"):\n",
    "        print(f\"正在處理頻道: {handle}\")\n",
    "        # 獲取頻道 ID\n",
    "        channel_id = get_channel_id_by_handle(handle)\n",
    "        if not channel_id:\n",
    "            print(f\"無法找到頻道: {handle}\")\n",
    "            continue\n",
    "\n",
    "        # 獲取頻道視頻數據\n",
    "        video_data = get_channel_video_data(channel_id)\n",
    "        if not video_data:\n",
    "            print(f\"未能獲取 {handle} 的任何視頻數據。\")\n",
    "            continue\n",
    "\n",
    "        # 獲取訂閱數\n",
    "        subscribers = get_channel_subscriber_count(channel_id)\n",
    "        for item in video_data:\n",
    "            item['subscribers'] = subscribers\n",
    "        # 將數據追加到總數據中\n",
    "        all_video_data.extend(video_data)\n",
    "\n",
    "    # 使用 tqdm 進度條寫入數據\n",
    "    print(\"正在保存數據到 CSV 文件...\")\n",
    "    with tqdm(total=len(all_video_data), desc=\"保存數據進度\", unit=\"影片\") as pbar:\n",
    "        if not os.path.exists(csv_file):\n",
    "            # 如果文件不存在，創建並寫入表頭\n",
    "            df = pd.DataFrame(all_video_data)\n",
    "            df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            # 如果文件存在，追加數據（避免重複）\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            new_df = pd.DataFrame(all_video_data)\n",
    "            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='video_id')\n",
    "            combined_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "\n",
    "        pbar.update(len(all_video_data))\n",
    "\n",
    "    print(\"所有數據已成功保存到 CSV 文件：\")\n",
    "    print(pd.read_csv(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/1 [00:00<?, ?頻道/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理頻道: @woshiquanshu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度: 100%|██████████| 1/1 [00:02<00:00,  2.91s/頻道]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在保存數據到 CSV 文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "保存數據進度: 100%|██████████| 5/5 [00:00<00:00, 1016.31影片/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有數據已成功保存到 CSV 文件：\n",
      "      video_id                            title   views  likes  comments  \\\n",
      "0  FgLkUspIsL0     全職獵人405話完整解說：西索本尊現身，旅團黑幫皆有行動  256441   2960       504   \n",
      "1  Fy_bHNbWdRU  拳叔講哆啦A夢：哆啦A夢把房子變身無敵列車頭！開著房子去兜風！  154404   1217        31   \n",
      "2  61fYLew16O8    臥底名單洩露，庫拉索的選擇，柯南劇場版20 純黑的噩夢解說   77427    653        69   \n",
      "3  SHyMV8kt3F0          獵人解說2:少數服從多數的陷阱，西索的放養計劃  239759   1263       150   \n",
      "4  Htyr-4aOqe0      一反常态的基德，解救业火中的向日葵，柯南剧场版19解说   71886    557        64   \n",
      "\n",
      "   publish_days  subscribers  \n",
      "0            35       261000  \n",
      "1          1849       261000  \n",
      "2           319       261000  \n",
      "3          1053       261000  \n",
      "4           325       261000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handles = ['@woshiquanshu']\n",
    "batch_main(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的 Handles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:   0%|          | 0/10 [00:00<?, ?頻道/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理頻道: @-df219\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  10%|█         | 1/10 [00:10<01:33, 10.35s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @-df219\n",
      "正在處理頻道: @-duoduolovestaiwan5616\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  20%|██        | 2/10 [00:20<01:21, 10.17s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @-duoduolovestaiwan5616\n",
      "正在處理頻道: @-miuraboilertaiwan3201\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  30%|███       | 3/10 [00:30<01:10, 10.11s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @-miuraboilertaiwan3201\n",
      "正在處理頻道: @-mosachianchian\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  40%|████      | 4/10 [00:40<01:00, 10.08s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @-mosachianchian\n",
      "正在處理頻道: @1000taiwan\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  50%|█████     | 5/10 [00:50<00:50, 10.07s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @1000taiwan\n",
      "正在處理頻道: @1001taiwanstories\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  60%|██████    | 6/10 [01:00<00:40, 10.06s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @1001taiwanstories\n",
      "正在處理頻道: @13妹在台灣\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  70%|███████   | 7/10 [01:10<00:30, 10.05s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @13妹在台灣\n",
      "正在處理頻道: @486系列之吃遍全台灣\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  80%|████████  | 8/10 [01:20<00:20, 10.05s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @486系列之吃遍全台灣\n",
      "正在處理頻道: @57gofun\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度:  90%|█████████ | 9/10 [01:30<00:10, 10.05s/頻道]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @57gofun\n",
      "正在處理頻道: @ablin\n",
      "请求受限，正在增加延时...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理頻道進度: 100%|██████████| 10/10 [01:40<00:00, 10.07s/頻道]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無法找到頻道: @ablin\n",
      "正在保存數據到 CSV 文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "保存數據進度: 0影片 [00:00, ?影片/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有數據已成功保存到 CSV 文件：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m handles \u001b[38;5;241m=\u001b[39m get_handles_from_keyword(keyword, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m找到的 Handles:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[43mbatch_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 48\u001b[0m, in \u001b[0;36mbatch_main\u001b[1;34m(handles, csv_file)\u001b[0m\n\u001b[0;32m     45\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(all_video_data))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m所有數據已成功保存到 CSV 文件：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jeffi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "'''import json\n",
    "import time \n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'AIzaSyAZioI1gbrRVHsB8Dylb96npimTouclBB8'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# 進度儲存檔案\n",
    "PROGRESS_FILE = 'search_progress.json'\n",
    "\n",
    "def save_progress(handles, next_page_token):\n",
    "    \"\"\"儲存搜尋進度到檔案\"\"\"\n",
    "    progress_data = {\n",
    "        \"handles\": sorted(list(handles)),  # 確保順序且無重複\n",
    "        \"nextPageToken\": next_page_token\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"載入搜尋進度\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "            return set(progress_data[\"handles\"]), progress_data[\"nextPageToken\"]\n",
    "    return set(), None\n",
    "\n",
    "def get_handles_from_keyword(keyword, max_results=50):\n",
    "    handles, next_page_token = load_progress()\n",
    "\n",
    "    while len(handles) < max_results:\n",
    "        # 搜尋頻道\n",
    "        search_request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='channel',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            channel_id = item['snippet']['channelId']\n",
    "\n",
    "            # 在每次调用频道 API 前加入延时\n",
    "            time.sleep(3.0)  # 延时 1.5 秒\n",
    "\n",
    "            # 獲取頻道資訊以提取 handle\n",
    "            channel_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            channel_response = channel_request.execute()\n",
    "\n",
    "            for channel in channel_response['items']:\n",
    "                custom_url = channel['snippet'].get('customUrl', None)\n",
    "                title = channel['snippet']['title']\n",
    "                handle = f\"@{custom_url}\" if custom_url else f\"@{title.replace(' ', '').replace('-', '')}\"\n",
    "                handles.add(handle)\n",
    "\n",
    "        # 儲存進度\n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        save_progress(handles, next_page_token)\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        # 在每次分页请求之间延时\n",
    "        time.sleep(3.0)  \n",
    "\n",
    "    return sorted(list(handles))[:max_results]\n",
    "\n",
    "\n",
    "# 搜尋台灣相關的頻道\n",
    "keyword = \"台灣\"\n",
    "handles = get_handles_from_keyword(keyword, max_results=10)\n",
    "print(\"找到的 Handles:\")\n",
    "batch_main(handles)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
